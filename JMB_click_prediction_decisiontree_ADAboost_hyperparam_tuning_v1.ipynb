{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "## Click Prediction Project\n",
    "\n",
    "Matt Leffers, John Burt\n",
    "\n",
    "This notebook uses hyperparameter tuning to optimize a decision tree with ADA boost click predictor.\n",
    "\n",
    "Here, I use GridSearchCV to tune the classifier hyperparams. I also added a pipeline, which  will make it easier to add tuning of different data selection and transformation schemes in the future.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading original pickled data...\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "location='./data/'\n",
    "clickfilename = 'train'\n",
    "\n",
    "converters = {\"site_id\": lambda x: int(x, 16),\n",
    "              \"site_domain\": lambda x: int(x, 16),\n",
    "              \"site_category\": lambda x: int(x, 16),\n",
    "              \"app_id\": lambda x: int(x, 16),\n",
    "              \"app_domain\": lambda x: int(x, 16),\n",
    "              \"app_category\": lambda x: int(x, 16),\n",
    "              \"device_id\": lambda x: int(x, 16),\n",
    "              \"device_model\": lambda x: int(x, 16),\n",
    "              \"device_type\": lambda x: int(x, 16),\n",
    "              \"device_ip\": lambda x: int(x, 16),\n",
    "             }\n",
    "\n",
    "clickcsvpath = location+clickfilename+'.csv'\n",
    "clickpicklepath = location+clickfilename+'.pkl'\n",
    "\n",
    "try:\n",
    "    print('reading original pickled data...')\n",
    "    with open(clickpicklepath, 'rb') as handle:\n",
    "        data = pickle.load(handle)\n",
    "\n",
    "except:\n",
    "    print('error: reading original csv file')\n",
    "    #Import csv file\n",
    "    data=pd.read_csv(clickcsvpath, converters=converters) \n",
    "    # save data\n",
    "    with open(clickpicklepath, 'wb') as handle:\n",
    "        pickle.dump(data, handle, protocol=pickle.HIGHEST_PROTOCOL)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Equalize number of click vs non-click samples for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original data = 40428967 rows: 6865066 clicks, 33563901 nonclicks 17.0% clicks\n",
      "training data = 1000000 rows, equal# clicks/nonclicks \n"
     ]
    }
   ],
   "source": [
    "# extract our X and y variables for training\n",
    "y = data['click'].copy()\n",
    "X = data[data.columns.values[2:]].copy()\n",
    "\n",
    "# from the larger dataset, subsample nsamps click and no-click records\n",
    "y0 = y[y==0]\n",
    "X0 = X[y==0]\n",
    "y1 = y[y==1]\n",
    "X1 = X[y==1]\n",
    "\n",
    "# nsamps = y1.shape[0] # use as many samples as possible\n",
    "nsamps = 500000\n",
    "\n",
    "print(\"original data = %d rows: %d clicks, %d nonclicks %1.1f%% clicks\"%(\n",
    "    y.shape[0], y1.shape[0], y0.shape[0], 100*y1.shape[0]/y.shape[0]))\n",
    "\n",
    "y_eq = y1[:nsamps].append(y0[:nsamps], ignore_index=True)\n",
    "X_eq = X1[:nsamps].append(X0[:nsamps], ignore_index=True)\n",
    "\n",
    "print(\"training data = %d rows, equal# clicks/nonclicks \"%(y_eq.shape[0]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "original data = 500000 rows: 82037 clicks, 417963 nonclicks 16.4% clicks\n",
    "training data = 164074 rows, equal# clicks/nonclicks "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing grid search...\n",
      "Fitting 3 folds for each of 9 candidates, totalling 27 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  27 out of  27 | elapsed: 90.5min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 5615.877s\n",
      "\n",
      "Best score: 0.716\n",
      "Best parameters set:\n",
      "\tclf__learning_rate: 0.5\n",
      "\tclf__n_estimators: 50\n"
     ]
    }
   ],
   "source": [
    "from time import time\n",
    "from scipy import stats\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "rng = np.random.RandomState(1)\n",
    "\n",
    "# Define SGDClassifier defaults: \n",
    "# define defaults: doing it this way allows us to define our own default params\n",
    "clf_defaults = {\n",
    "    'base_estimator' : DecisionTreeClassifier(max_depth=4),\n",
    "    'n_estimators' : 100, \n",
    "    'learning_rate' : 1,\n",
    "    }\n",
    "\n",
    "# Create a pipeline, allowing to tune a transformer and the SGDClassifier classifier.\n",
    "# (transformer not implemented yet)\n",
    "pipeline = Pipeline([   \n",
    "    ('clf', AdaBoostClassifier(**clf_defaults, random_state=rng))\n",
    "])\n",
    "\n",
    "# Define the parameters and values we want to test.\n",
    "# Uncommenting more parameters will give better exploring power but will\n",
    "#   increase processing time in a combinatorial way. I suggest tuning <= 3\n",
    "#   parameters at a time.\n",
    "# Note the naming format: pipelineobjectname__paramname\n",
    "parameters = {\n",
    "    'clf__n_estimators': (50,100,1000),\n",
    "    'clf__learning_rate': (.5, 1, 2),\n",
    "}\n",
    "\n",
    "# Create the grid search object.\n",
    "# Note that \"n_jobs=-1\" means that the search will use all of the \n",
    "#  computer's available processing cores to speed things up.\n",
    "grid_search = GridSearchCV(pipeline, parameters, n_jobs=-1, verbose=1)\n",
    "\n",
    "print(\"Performing grid search...\")\n",
    "# print(\"parameters:\")\n",
    "# print(parameters)\n",
    "t0 = time()\n",
    "\n",
    "# Run the grid search to find the best parameters for the classifier.\n",
    "grid_search.fit(X_eq, y_eq)\n",
    "\n",
    "print(\"done in %0.3fs\" % (time() - t0))\n",
    "print()\n",
    "\n",
    "print(\"Best score: %0.3f\" % grid_search.best_score_)\n",
    "print(\"Best parameters set:\")\n",
    "best_parameters = grid_search.best_estimator_.get_params()\n",
    "for param_name in sorted(parameters.keys()):\n",
    "    print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
